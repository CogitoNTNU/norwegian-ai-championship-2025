{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6cc0993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep modules up to date every time you hit Shift-Enter \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9051803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WANDB_API_KEY: 494a...\n",
      "MONAI version: 1.4.0\n",
      "Numpy version: 1.26.4\n",
      "Pytorch version: 2.7.1+cpu\n",
      "MONAI flags: HAS_EXT = False, USE_COMPILED = False, USE_META_DICT = False\n",
      "MONAI rev id: 46a5272196a6c2590ca2589029eed8e4d56ff008\n",
      "MONAI __file__: c:\\Users\\<username>\\Documents\\NM-AI\\norwegian-ai-championship-2025\\segmentation\\.venv\\Lib\\site-packages\\monai\\__init__.py\n",
      "\n",
      "Optional dependencies:\n",
      "Pytorch Ignite version: 0.4.11\n",
      "ITK version: 5.4.4\n",
      "Nibabel version: 5.3.2\n",
      "scikit-image version: 0.25.2\n",
      "scipy version: 1.16.1\n",
      "Pillow version: 11.3.0\n",
      "Tensorboard version: 2.20.0\n",
      "gdown version: 5.2.0\n",
      "TorchVision version: 0.22.1+cpu\n",
      "tqdm version: 4.67.1\n",
      "lmdb version: 1.7.3\n",
      "psutil version: 7.0.0\n",
      "pandas version: 2.3.1\n",
      "einops version: 0.8.1\n",
      "transformers version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "mlflow version: 3.1.4\n",
      "pynrrd version: 1.1.3\n",
      "clearml version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "\n",
      "For details about installing the optional dependencies, please visit:\n",
      "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtobiasfremming\u001b[0m (\u001b[33mnm-i-ki\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\tobia\\Documents\\NM-AI\\norwegian-ai-championship-2025\\segmentation\\wandb\\run-20250803_170427-7ue8cmke</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nm-i-ki/tumor-segmentation/runs/7ue8cmke' target=\"_blank\">unet_20250803_170427</a></strong> to <a href='https://wandb.ai/nm-i-ki/tumor-segmentation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nm-i-ki/tumor-segmentation' target=\"_blank\">https://wandb.ai/nm-i-ki/tumor-segmentation</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nm-i-ki/tumor-segmentation/runs/7ue8cmke' target=\"_blank\">https://wandb.ai/nm-i-ki/tumor-segmentation/runs/7ue8cmke</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 426 control images.\n",
      "Found 182 patient images.\n",
      "Randomly selected 182 control samples from 426 available.\n",
      "Final dataset: 182 patients + 182 controls = 364 samples (50/50 split)\n",
      "Amount of images train: 291 val: 73\n",
      "Starting training...\n",
      "\n",
      "Epoch 1/10\n",
      "  Train avg loss: 0.8433\n",
      "\n",
      "Epoch 2/10\n",
      "  Train avg loss: 0.8118\n",
      "  Val mean Dice: 0.0000\n",
      "  Best model saved with Dice 0.0000 at epoch 2\n",
      "\n",
      "Epoch 3/10\n",
      "  Train avg loss: 0.8040\n",
      "\n",
      "Epoch 4/10\n",
      "  Train avg loss: 0.8122\n",
      "  Val mean Dice: 0.0000\n",
      "\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 252\u001b[39m\n\u001b[32m    250\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mWANDB_API_KEY not found in environment variables. Please set it in .env file.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    251\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mWANDB_API_KEY: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mWANDB_API_KEY[:\u001b[32m4\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)  \u001b[38;5;66;03m# Print only the first 4 characters for security\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m252\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m../data/raw/tumor-segmentation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 172\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m(dataset_dir)\u001b[39m\n\u001b[32m    170\u001b[39m model.train()\n\u001b[32m    171\u001b[39m epoch_loss = \u001b[32m0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m step, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader, \u001b[32m1\u001b[39m):\n\u001b[32m    173\u001b[39m     imgs, segs = batch[\u001b[33m\"\u001b[39m\u001b[33mimg\u001b[39m\u001b[33m\"\u001b[39m].to(device), batch[\u001b[33m\"\u001b[39m\u001b[33mseg\u001b[39m\u001b[33m\"\u001b[39m].to(device)\n\u001b[32m    174\u001b[39m     optimizer.zero_grad()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tobia\\Documents\\NM-AI\\norwegian-ai-championship-2025\\segmentation\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:493\u001b[39m, in \u001b[36mDataLoader.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    491\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._iterator\n\u001b[32m    492\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m493\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tobia\\Documents\\NM-AI\\norwegian-ai-championship-2025\\segmentation\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:424\u001b[39m, in \u001b[36mDataLoader._get_iterator\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    422\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    423\u001b[39m     \u001b[38;5;28mself\u001b[39m.check_worker_number_rationality()\n\u001b[32m--> \u001b[39m\u001b[32m424\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tobia\\Documents\\NM-AI\\norwegian-ai-championship-2025\\segmentation\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1171\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter.__init__\u001b[39m\u001b[34m(self, loader)\u001b[39m\n\u001b[32m   1164\u001b[39m w.daemon = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1165\u001b[39m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[32m   1166\u001b[39m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[32m   1167\u001b[39m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[32m   1168\u001b[39m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[32m   1169\u001b[39m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[32m   1170\u001b[39m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1171\u001b[39m \u001b[43mw\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1172\u001b[39m \u001b[38;5;28mself\u001b[39m._index_queues.append(index_queue)\n\u001b[32m   1173\u001b[39m \u001b[38;5;28mself\u001b[39m._workers.append(w)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\uv\\python\\cpython-3.11.13-windows-x86_64-none\\Lib\\multiprocessing\\process.py:121\u001b[39m, in \u001b[36mBaseProcess.start\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process._config.get(\u001b[33m'\u001b[39m\u001b[33mdaemon\u001b[39m\u001b[33m'\u001b[39m), \\\n\u001b[32m    119\u001b[39m        \u001b[33m'\u001b[39m\u001b[33mdaemonic processes are not allowed to have children\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    120\u001b[39m _cleanup()\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m \u001b[38;5;28mself\u001b[39m._popen = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[38;5;28mself\u001b[39m._sentinel = \u001b[38;5;28mself\u001b[39m._popen.sentinel\n\u001b[32m    123\u001b[39m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[32m    124\u001b[39m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\uv\\python\\cpython-3.11.13-windows-x86_64-none\\Lib\\multiprocessing\\context.py:224\u001b[39m, in \u001b[36mProcess._Popen\u001b[39m\u001b[34m(process_obj)\u001b[39m\n\u001b[32m    222\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m    223\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_Popen\u001b[39m(process_obj):\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mProcess\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\uv\\python\\cpython-3.11.13-windows-x86_64-none\\Lib\\multiprocessing\\context.py:336\u001b[39m, in \u001b[36mSpawnProcess._Popen\u001b[39m\u001b[34m(process_obj)\u001b[39m\n\u001b[32m    333\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m    334\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_Popen\u001b[39m(process_obj):\n\u001b[32m    335\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpopen_spawn_win32\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[32m--> \u001b[39m\u001b[32m336\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\uv\\python\\cpython-3.11.13-windows-x86_64-none\\Lib\\multiprocessing\\popen_spawn_win32.py:95\u001b[39m, in \u001b[36mPopen.__init__\u001b[39m\u001b[34m(self, process_obj)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     94\u001b[39m     reduction.dump(prep_data, to_child)\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m     \u001b[43mreduction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_child\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     97\u001b[39m     set_spawning_popen(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\uv\\python\\cpython-3.11.13-windows-x86_64-none\\Lib\\multiprocessing\\reduction.py:60\u001b[39m, in \u001b[36mdump\u001b[39m\u001b[34m(obj, file, protocol)\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdump\u001b[39m(obj, file, protocol=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m     59\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m     \u001b[43mForkingPickler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import logging\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import monai\n",
    "from monai.data import list_data_collate, decollate_batch, DataLoader\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.transforms import (\n",
    "    Activations,\n",
    "    AsDiscrete,\n",
    "    Compose,\n",
    ")\n",
    "from monai.visualize import plot_2d_or_3d_image\n",
    "import wandb\n",
    "from tumor_dataset import create_tumor_dataset\n",
    "from monai.transforms import (\n",
    "    EnsureChannelFirstd,\n",
    "    Transform,\n",
    "    Compose,\n",
    "    LoadImaged,\n",
    "    RandCropByPosNegLabeld,\n",
    "    ScaleIntensityd,\n",
    "    # Spatial transforms\n",
    "    RandRotated,\n",
    "    RandFlipd,\n",
    "    RandZoomd,\n",
    "    RandShiftIntensityd,\n",
    "    RandAdjustContrastd,\n",
    "    RandGaussianNoised,\n",
    "    RandGaussianSmoothd,\n",
    "    RandScaleIntensityd,\n",
    "    # Intensity transforms\n",
    "    RandBiasFieldd,\n",
    "    RandGibbsNoised,\n",
    "    # Elastic deformation\n",
    "    Rand3DElasticd,\n",
    "    RandAffined,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "def main(dataset_dir):\n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "    monai.config.print_config()\n",
    "    \n",
    "    NUM_EPOCHS = 10\n",
    "    BATCH_SIZE = 2\n",
    "    LR = 1e-3\n",
    "    PATCH_SIZE = (96, 96)\n",
    "\n",
    "    # W&B run initialization\n",
    "    run = wandb.init(\n",
    "        project=\"tumor-segmentation\",\n",
    "        name=f\"unet_{datetime.now():%Y%m%d_%H%M%S}\",\n",
    "        config=dict(\n",
    "            num_epochs=NUM_EPOCHS,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            learning_rate=LR,\n",
    "            patch_size=PATCH_SIZE,\n",
    "            architecture=\"UNet\",\n",
    "            loss=\"Dice(sigmoid=True)\",\n",
    "            optimizer=\"Adam\",\n",
    "        ),\n",
    "        save_code=True,\n",
    "        sync_tensorboard=True,\n",
    "        tags=[\"monai\", \"segmentation\"],\n",
    "    )\n",
    "    \n",
    "    train_augmentations = [\n",
    "        # Spatial augmentations (applied to both image and mask)\n",
    "        # RandFlipd(keys=[\"img\", \"seg\"], prob=0.5, spatial_axis=0),  # Horizontal flip\n",
    "        # RandFlipd(keys=[\"img\", \"seg\"], prob=0.5, spatial_axis=1),  # Vertical flip\n",
    "        \n",
    "        RandRotated(\n",
    "            keys=[\"img\", \"seg\"],\n",
    "            range_x=0.15,  # Â±11.5 degrees\n",
    "            prob=0.3,\n",
    "            keep_size=True,\n",
    "            mode=[\"bilinear\", \"nearest\"],  # bilinear for image, nearest for mask\n",
    "        ),\n",
    "        \n",
    "        RandZoomd(\n",
    "            keys=[\"img\", \"seg\"],\n",
    "            min_zoom=0.9,\n",
    "            max_zoom=1.1,\n",
    "            prob=0.3,\n",
    "            mode=[\"bilinear\", \"nearest\"],\n",
    "        ),\n",
    "        \n",
    "        RandAffined(\n",
    "            keys=[\"img\", \"seg\"],\n",
    "            rotate_range=(0.1, 0.1),  # Small rotations\n",
    "            scale_range=(0.1, 0.1),   # Small scaling\n",
    "            translate_range=(10, 10), # Small translations\n",
    "            prob=0.3,\n",
    "            mode=[\"bilinear\", \"nearest\"],\n",
    "        ),\n",
    "        \n",
    "        # Intensity augmentations (applied only to image)\n",
    "        RandShiftIntensityd(keys=[\"img\"], offsets=0.1, prob=0.3),\n",
    "        \n",
    "        RandAdjustContrastd(keys=[\"img\"], gamma=(0.8, 1.2), prob=0.3),\n",
    "        \n",
    "        RandScaleIntensityd(keys=[\"img\"], factors=0.1, prob=0.3),\n",
    "        \n",
    "        RandGaussianNoised(keys=[\"img\"], std=0.01, prob=0.2),\n",
    "        \n",
    "        RandGaussianSmoothd(\n",
    "            keys=[\"img\"],\n",
    "            sigma_x=(0.5, 1.0),\n",
    "            sigma_y=(0.5, 1.0),\n",
    "            prob=0.2,\n",
    "        ),\n",
    "        \n",
    "        # Medical imaging specific augmentations\n",
    "        RandBiasFieldd(keys=[\"img\"], degree=3, coeff_range=(0.0, 0.1), prob=0.2),\n",
    "        \n",
    "        RandGibbsNoised(keys=[\"img\"], alpha=(0.0, 0.5), prob=0.1),\n",
    "    ]\n",
    "\n",
    "    # Create datasets\n",
    "    train_ds, val_ds = create_tumor_dataset(dataset_dir=dataset_dir, train_data_augmentation=train_augmentations)\n",
    "\n",
    "    print(f\"Amount of images train: {len(train_ds)} val: {len(val_ds)}\")\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=4,\n",
    "        collate_fn=list_data_collate,\n",
    "        pin_memory=torch.cuda.is_available(),\n",
    "    )\n",
    "    val_loader = DataLoader(val_ds, batch_size=1, num_workers=4, collate_fn=list_data_collate)\n",
    "\n",
    "    # Model, loss, optimizer\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = monai.networks.nets.SegResNet(\n",
    "        spatial_dims=2,\n",
    "        init_filters=32,\n",
    "        in_channels=4,\n",
    "        out_channels=4,\n",
    "        blocks_down=[1, 2, 2, 4],\n",
    "        blocks_up=[1, 1, 1],\n",
    "    ).to(device)\n",
    "    loss_function = monai.losses.DiceLoss(sigmoid=True)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), LR)\n",
    "\n",
    "    # Attach gradients & parameters to W&B\n",
    "    wandb.watch(model, log=\"all\", log_freq=10)\n",
    "\n",
    "    # Train model\n",
    "    print(\"Starting training...\")\n",
    "    dice_metric = DiceMetric(include_background=True, reduction=\"mean\")\n",
    "    post_trans = Compose([Activations(sigmoid=True), AsDiscrete(threshold=0.5)])\n",
    "    tensorboard_writer = SummaryWriter()\n",
    "    best_metric = -1.0\n",
    "    best_epoch = -1\n",
    "\n",
    "    os.makedirs(\"models\", exist_ok=True)\n",
    "    global_step = 0\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{NUM_EPOCHS}\")\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for step, batch in enumerate(train_loader, 1):\n",
    "            imgs, segs = batch[\"img\"].to(device), batch[\"seg\"].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_function(outputs, segs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            # Log per step\n",
    "            global_step += 1\n",
    "            wandb.log({\"train_loss\": loss.item(), \"epoch\": epoch + 1, \"step\": global_step})\n",
    "            tensorboard_writer.add_scalar(\n",
    "                \"train_loss\", loss.item(), epoch * len(train_loader) + step\n",
    "            )\n",
    "        avg_epoch_loss = epoch_loss / step\n",
    "        print(f\"  Train avg loss: {avg_epoch_loss:.4f}\")\n",
    "        wandb.log({\"train_avg_loss\": avg_epoch_loss, \"epoch\": epoch + 1})\n",
    "\n",
    "        if (epoch + 1) % 2 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for val_batch in val_loader:\n",
    "                    val_imgs, val_segs = val_batch[\"img\"].to(device), val_batch[\"seg\"].to(device)\n",
    "                    sw_out = sliding_window_inference(val_imgs, PATCH_SIZE, 4, model)\n",
    "                    preds = [post_trans(x) for x in decollate_batch(sw_out)]\n",
    "                    dice_metric(y_pred=preds, y=val_segs)\n",
    "                metric = dice_metric.aggregate().item()\n",
    "                dice_metric.reset()\n",
    "                tensorboard_writer.add_scalar(\"val_mean_dice\", metric, epoch + 1)\n",
    "                print(f\"  Val mean Dice: {metric:.4f}\")\n",
    "\n",
    "                # Log validation metric & sample images\n",
    "                wandb.log({\"val_mean_dice\": metric, \"epoch\": epoch + 1})\n",
    "\n",
    "                # Log first channel of first image / pred / label as examples\n",
    "                img_np = val_imgs[0, 0].cpu().float().numpy()\n",
    "                pred_np = preds[0][0].cpu().float().numpy()\n",
    "                label_np = val_segs[0, 0].cpu().float().numpy()\n",
    "                wandb.log(\n",
    "                    {\n",
    "                        \"example_input\": wandb.Image(img_np, caption=\"input\"),\n",
    "                        \"example_pred\": wandb.Image(pred_np, caption=\"prediction\"),\n",
    "                        \"example_label\": wandb.Image(label_np, caption=\"ground truth\"),\n",
    "                        \"epoch\": epoch + 1,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                if metric > best_metric:\n",
    "                    best_metric = metric\n",
    "                    best_epoch = epoch + 1\n",
    "                    best_model_path = f\"models/best_model_{metric:.4f}.pth\"\n",
    "                    torch.save(model.state_dict(), best_model_path)\n",
    "                    print(\n",
    "                        f\"  Best model saved with Dice {best_metric:.4f} at epoch {best_epoch}\"\n",
    "                    )\n",
    "\n",
    "                    # Save model to W&B as an artifact\n",
    "                    artifact = wandb.Artifact(\"best_model\", type=\"model\")\n",
    "                    artifact.add_file(best_model_path)\n",
    "                    run.log_artifact(artifact)\n",
    "\n",
    "                # Continue logging to TensorBoard if desired\n",
    "                plot_2d_or_3d_image(val_imgs, epoch + 1, tensorboard_writer, index=0, tag=\"image\")\n",
    "                plot_2d_or_3d_image(val_segs, epoch + 1, tensorboard_writer, index=0, tag=\"label\")\n",
    "                plot_2d_or_3d_image(preds, epoch + 1, tensorboard_writer, index=0, tag=\"output\")\n",
    "\n",
    "    print(\n",
    "        f\"\\nTraining done! Best Dice {best_metric:.4f} reached on epoch {best_epoch}\"\n",
    "    )\n",
    "    tensorboard_writer.close()\n",
    "    run.finish()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    load_dotenv()  # Load environment variables from .env file\n",
    "    WANDB_API_KEY = os.getenv(\"WANDB_API_KEY\")\n",
    "    if not WANDB_API_KEY:\n",
    "        raise ValueError(\"WANDB_API_KEY not found in environment variables. Please set it in .env file.\")\n",
    "    print(f\"WANDB_API_KEY: {WANDB_API_KEY[:4]}...\")  # Print only the first 4 characters for security\n",
    "    main(\"../data/raw/tumor-segmentation\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tumor-segmentation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
