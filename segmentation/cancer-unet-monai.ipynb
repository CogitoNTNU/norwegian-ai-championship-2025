{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6cc0993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep modules up to date every time you hit Shift-Enter \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9051803",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorboard'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msys\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtensorboard\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SummaryWriter\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmonai\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmonai\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m list_data_collate, decollate_batch, DataLoader\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/nm-ai/.venv/lib/python3.12/site-packages/torch/utils/tensorboard/__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorboard\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_vendor\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpackaging\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mversion\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Version\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(tensorboard, \u001b[33m\"\u001b[39m\u001b[33m__version__\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m Version(\n\u001b[32m      5\u001b[39m     tensorboard.__version__\n\u001b[32m      6\u001b[39m ) < Version(\u001b[33m\"\u001b[39m\u001b[33m1.15\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tensorboard'"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import monai\n",
    "from monai.data import list_data_collate, decollate_batch, DataLoader\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.transforms import (\n",
    "    Activations,\n",
    "    AsDiscrete,\n",
    "    Compose,\n",
    ")\n",
    "from monai.visualize import plot_2d_or_3d_image\n",
    "from tumor_dataset import create_tumor_dataset\n",
    "\n",
    "def main(dataset_dir):\n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "    monai.config.print_config()\n",
    "    \n",
    "    NUM_EPOCHS = 10\n",
    "    BATCH_SIZE = 2\n",
    "\n",
    "    # Create datasets\n",
    "    train_ds, val_ds, = create_tumor_dataset(\n",
    "        dataset_dir=dataset_dir,\n",
    "    )\n",
    "\n",
    "    print(f\"Amount of images train: {len(train_ds)} val: {len(val_ds)}\")\n",
    "    train_loader = DataLoader(\n",
    "        train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "        num_workers=4, collate_fn=list_data_collate,\n",
    "        pin_memory=torch.cuda.is_available(),\n",
    "    )\n",
    "    val_loader = DataLoader(val_ds, batch_size=1, num_workers=4, collate_fn=list_data_collate)\n",
    "\n",
    "    # Model, loss, optimizer\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = monai.networks.nets.UNet(\n",
    "        spatial_dims=2, in_channels=4, out_channels=4,\n",
    "        channels=(16,32,64,128,256), strides=(2,2,2,2),\n",
    "        num_res_units=2,\n",
    "    ).to(device)\n",
    "    loss_function = monai.losses.DiceLoss(sigmoid=True)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), 1e-3)\n",
    "\n",
    "    # Train model\n",
    "    print(\"Starting training...\")\n",
    "    dice_metric = DiceMetric(include_background=True, reduction=\"mean\")\n",
    "    post_trans = Compose([Activations(sigmoid=True), AsDiscrete(threshold=0.5)])\n",
    "    writer = SummaryWriter()\n",
    "    best_metric = -1.0\n",
    "    best_epoch = -1\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        print(f\"\\nEpoch {epoch+1}/10\")\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for step, batch in enumerate(train_loader, 1):\n",
    "            imgs, segs = batch[\"img\"].to(device), batch[\"seg\"].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_function(outputs, segs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            writer.add_scalar(\"train_loss\", loss.item(), epoch * len(train_loader) + step)\n",
    "        print(f\"  Train avg loss: {epoch_loss/step:.4f}\")\n",
    "\n",
    "        if (epoch+1) % 2 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for val_batch in val_loader:\n",
    "                    val_imgs, val_segs = val_batch[\"img\"].to(device), val_batch[\"seg\"].to(device)\n",
    "                    sw_out = sliding_window_inference(val_imgs, (96,96), 4, model)\n",
    "                    preds = [post_trans(x) for x in decollate_batch(sw_out)]\n",
    "                    dice_metric(y_pred=preds, y=val_segs)\n",
    "                metric = dice_metric.aggregate().item()\n",
    "                dice_metric.reset()\n",
    "                writer.add_scalar(\"val_mean_dice\", metric, epoch+1)\n",
    "                print(f\"  Val mean Dice: {metric:.4f}\")\n",
    "\n",
    "                if metric > best_metric:\n",
    "                    best_metric = metric\n",
    "                    best_epoch = epoch+1\n",
    "                    torch.save(model.state_dict(), f\"models/best_model_{metric}.pth\")\n",
    "                    print(f\"  Best model saved with Dice {best_metric:.4f} at epoch {best_epoch}\")\n",
    "                # Log images\n",
    "                plot_2d_or_3d_image(val_imgs, epoch+1, writer, index=0, tag=\"image\")\n",
    "                plot_2d_or_3d_image(val_segs, epoch+1, writer, index=0, tag=\"label\")\n",
    "                plot_2d_or_3d_image(preds, epoch+1, writer, index=0, tag=\"output\")\n",
    "\n",
    "    print(f\"\\nTraining done! Beste Dice {best_metric:.4f} reached on epoch {best_epoch}\")\n",
    "    writer.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(\"../data/raw/tumor-segmentation\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
