# Embedding Models Configuration

default_model: "nomic-embed-text-v1.5"

models:
  # Fast and efficient baseline
  all-MiniLM-L6-v2:
    dimension: 384
    max_seq_length: 256
    description: "Fast, lightweight model - good baseline"
    use_cases:
      - "Quick prototyping"
      - "Low-resource environments"
    
  # High-quality general purpose
  all-mpnet-base-v2:
    dimension: 768
    max_seq_length: 384
    description: "High-quality general embeddings"
    use_cases:
      - "General semantic search"
      - "Better accuracy than MiniLM"
  
  # Matryoshka model with flexible dimensions
  nomic-embed-text-v1.5:
    dimension: 768
    max_seq_length: 8192
    matryoshka_dims: [768, 512, 256, 128, 64]
    description: "Flexible dimensions, long context"
    use_cases:
      - "Variable performance/speed trade-offs"
      - "Long medical documents"
      - "Memory-constrained deployments"
  
  # Alibaba GTE models
  gte-base:
    dimension: 768
    max_seq_length: 512
    description: "Strong performance on medical tasks"
    use_cases:
      - "Medical text retrieval"
      - "Good baseline for fine-tuning"
  
  gte-large:
    dimension: 1024
    max_seq_length: 512
    description: "Larger GTE model for better accuracy"
    use_cases:
      - "When accuracy is priority"
      - "Sufficient compute resources"
  
  # BAAI BGE models
  bge-base-en-v1.5:
    dimension: 768
    max_seq_length: 512
    description: "Excellent retrieval performance"
    use_cases:
      - "Document retrieval"
      - "Question answering"
  
  bge-large-en-v1.5:
    dimension: 1024
    max_seq_length: 512
    description: "Top-tier retrieval model"
    use_cases:
      - "Best retrieval accuracy"
      - "Production systems"
  
  # Medical-specific models
  pubmedbert-base-embeddings:
    dimension: 768
    max_seq_length: 512
    description: "Fine-tuned on PubMed literature"
    use_cases:
      - "Medical literature search"
      - "Clinical text processing"
    preprocessing: true
  
  BioLORD-2023:
    dimension: 768
    max_seq_length: 512
    description: "Biomedical embeddings with knowledge graph"
    use_cases:
      - "Biomedical concept matching"
      - "Medical entity linking"
    preprocessing: true

# Model selection criteria
selection_criteria:
  speed_priority:
    - "all-MiniLM-L6-v2"
    - "nomic-embed-text-v1.5"  # with reduced dimensions
    - "gte-base"
  
  accuracy_priority:
    - "bge-large-en-v1.5"
    - "gte-large"
    - "nomic-embed-text-v1.5"  # full dimensions
  
  medical_domain:
    - "pubmedbert-base-embeddings"
    - "BioLORD-2023"
    - "gte-base"  # good general model for medical
  
  balanced:
    - "nomic-embed-text-v1.5"
    - "bge-base-en-v1.5"
    - "all-mpnet-base-v2"