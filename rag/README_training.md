# Medical Embedding Model Training

## ‚úÖ Dataset Successfully Created!

Your medical embedding dataset has been successfully generated with the following statistics:

- **Total examples**: 9,450 training pairs
- **Positive examples**: 1,890 (anchor-positive pairs)
- **Negative examples**: 7,560 (anchor-negative pairs)
- **Positive ratio**: 20% (optimized for embedding training)
- **Total statements**: 630 medical statements
- **Topics covered**: 115 medical topics
- **Total chunks**: 2,277 medical document chunks
- **Token optimization**: All texts truncated to ‚â§384 tokens for BERT compatibility

The dataset is saved as `medical_embedding_dataset.json` and is ready for training!

## üêç Python Environment Issue

Unfortunately, there's a pandas C extension compilation issue in your current environment that prevents running the SentenceTransformers training scripts. This is a known issue on some Windows systems.

## üöÄ Recommended Training Solutions

### Option 1: Google Colab (Recommended)
The easiest way to train your model is using Google Colab:

1. Upload `medical_embedding_dataset.json` to Google Colab
2. Use the notebook provided: `FT_Embedding_Models_on_Domain_Specific_Data.ipynb`
3. Modify it to load your dataset instead of the legal dataset

**Colab Training Code:**
```python
# Upload medical_embedding_dataset.json to Colab first

import json
from sentence_transformers import SentenceTransformer, SentenceTransformerModelCardData, SentenceTransformerTrainingArguments, SentenceTransformerTrainer
from sentence_transformers.evaluation import InformationRetrievalEvaluator, SequentialEvaluator
from sentence_transformers.util import cos_sim
from sentence_transformers.losses import MatryoshkaLoss, MultipleNegativesRankingLoss
from sentence_transformers.training_args import BatchSamplers
from datasets import Dataset

# Load your medical dataset
with open('medical_embedding_dataset.json', 'r') as f:
    data = json.load(f)

# Convert to training format
train_examples = []
for example in data['examples']:
    train_examples.append({
        'anchor': example['anchor'],
        'positive': example['positive']
    })

train_dataset = Dataset.from_list(train_examples)

# Load BioBERT model
model_id = "pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb"
model = SentenceTransformer(
    model_id,
    model_kwargs={"attn_implementation": "sdpa"},
    model_card_data=SentenceTransformerModelCardData(
        language="en",
        license="apache-2.0",
        model_name="BioBERT Medical Domain Embeddings with Matryoshka",
    ),
)

# Setup training
matryoshka_dimensions = [768, 512, 256, 128, 64]
base_loss = MultipleNegativesRankingLoss(model)
train_loss = MatryoshkaLoss(model, base_loss, matryoshka_dims=matryoshka_dimensions)

# Training arguments
args = SentenceTransformerTrainingArguments(
    output_dir="biobert-medical-embeddings-mrl",
    num_train_epochs=4,
    per_device_train_batch_size=16,
    gradient_accumulation_steps=32,
    per_device_eval_batch_size=8,
    warmup_ratio=0.1,
    learning_rate=2e-5,
    lr_scheduler_type="cosine",
    optim="adamw_torch_fused",
    tf32=True,
    bf16=True,
    batch_sampler=BatchSamplers.NO_DUPLICATES,
    eval_strategy="epoch",
    save_strategy="epoch",
    logging_steps=10,
    save_total_limit=3,
    load_best_model_at_end=True,
    metric_for_best_model="eval_dim_256_cosine_ndcg@10",
)

# Create trainer and train
trainer = SentenceTransformerTrainer(
    model=model,
    args=args,
    train_dataset=train_dataset.select_columns(["positive", "anchor"]),
    loss=train_loss,
)

trainer.train()
trainer.save_model()

# Upload to Hugging Face Hub (optional)
# trainer.model.push_to_hub("your-username/biobert-medical-embeddings")
```

### Option 2: Docker Environment
Create a clean Docker environment:

```dockerfile
FROM python:3.11-slim

RUN pip install sentence-transformers datasets torch transformers
COPY medical_embedding_dataset.json /app/
COPY train_medical_embeddings.py /app/
WORKDIR /app

CMD ["python", "train_medical_embeddings.py"]
```

### Option 3: Clean Conda Environment
```bash
conda create -n medical-embeddings python=3.11
conda activate medical-embeddings
pip install sentence-transformers datasets torch transformers
python train_medical_embeddings.py
```

### Option 4: Use the Basic Transformers Script
If you can fix the pandas issue in your environment, you have three training scripts ready:

1. **`train_medical_embeddings.py`** - Full-featured with Matryoshka training and comprehensive evaluation
2. **`train_medical_embeddings_simple.py`** - Simplified version with basic functionality
3. **`train_medical_embeddings_basic.py`** - Uses transformers directly, minimal dependencies

## üìä Expected Results

Based on the notebook example, you should expect:
- **40-60% improvement** in retrieval metrics (NDCG@10, MRR@10, MAP@100)
- **Better domain-specific understanding** for medical queries
- **Multi-dimensional embeddings** (768d, 512d, 256d, 128d, 64d) for different performance/efficiency trade-offs

## üéØ Model Usage After Training

Once trained, you can use your model like this:

```python
from sentence_transformers import SentenceTransformer

# Load your fine-tuned model
model = SentenceTransformer("biobert-medical-embeddings-mrl", truncate_dim=256)

# Encode medical texts
medical_texts = [
    "Patient presents with chest pain and shortness of breath",
    "Acute myocardial infarction with ST elevation",
    "Normal cardiac function on echocardiogram"
]

embeddings = model.encode(medical_texts)
similarities = model.similarity(embeddings, embeddings)
print(similarities)
```

## üìÅ Files Created

- ‚úÖ `medical_embedding_dataset.json` - Your optimized training dataset (9,450 examples)
- ‚úÖ `train_medical_embeddings.py` - Full training script with Matryoshka learning
- ‚úÖ `train_medical_embeddings_simple.py` - Simplified training script  
- ‚úÖ `train_medical_embeddings_basic.py` - Basic transformers-only script
- ‚úÖ `FT_Embedding_Models_on_Domain_Specific_Data.ipynb` - Reference notebook

## üîß Next Steps

1. **Immediate**: Use Google Colab with the provided code above
2. **Alternative**: Set up a clean Python environment and run one of the training scripts
3. **Integration**: Once trained, integrate the model into your RAG pipeline by updating `rag-pipeline/document_store_embeddings.py`

Your dataset is perfectly optimized for medical domain embedding training with the BioBERT model you specified!